{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "project Flow\n",
    "\n",
    "* Load Documents\n",
    "* Chunk Documents\n",
    "* Clean Up docs from irrelavant text\n",
    "* Create a vector database from documents\n",
    "* Create Retriver\n",
    "* Create Retriver engine from and retriver and knowledge base\n",
    "* write a prompt with context from retriver engine and user query\n",
    "* Load pretrained model and embeddings\n",
    "* pass prompt to pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama_index==0.10.19 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (0.10.19)\n",
      "Requirement already satisfied: llama_index_core==0.10.19 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (0.10.19)\n",
      "Requirement already satisfied: torch in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: llama-index-embeddings-huggingface in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (0.2.3)\n",
      "Requirement already satisfied: peft in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: optimum in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (1.23.3)\n",
      "Requirement already satisfied: bitsandbytes in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (0.44.1)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse<0.2.0,>=0.1.2 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama_index==0.10.19) (0.1.6)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.2 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama_index==0.10.19) (0.1.3)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.5 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama_index==0.10.19) (0.1.11)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama_index==0.10.19) (0.1.6)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama_index==0.10.19) (0.1.9)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.2.0,>=0.1.4 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama_index==0.10.19) (0.1.7)\n",
      "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.3 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama_index==0.10.19) (0.1.6)\n",
      "Requirement already satisfied: llama-index-cli<0.2.0,>=0.1.2 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama_index==0.10.19) (0.1.13)\n",
      "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama_index==0.10.19) (0.9.48.post4)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.2.0,>=0.1.5 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama_index==0.10.19) (0.1.11)\n",
      "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.4 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama_index==0.10.19) (0.1.22)\n",
      "Requirement already satisfied: pandas in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama_index_core==0.10.19) (2.2.3)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama_index_core==0.10.19) (4.12.2)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama_index_core==0.10.19) (0.8.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama_index_core==0.10.19) (3.11.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama_index_core==0.10.19) (2024.9.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama_index_core==0.10.19) (2.32.3)\n",
      "Requirement already satisfied: httpx in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama_index_core==0.10.19) (0.28.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama_index_core==0.10.19) (1.6.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama_index_core==0.10.19) (8.5.0)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama_index_core==0.10.19) (1.2.15)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama_index_core==0.10.19) (3.9.1)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama_index_core==0.10.19) (0.6.7)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama_index_core==0.10.19) (1.0.8)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama_index_core==0.10.19) (3.4.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama_index_core==0.10.19) (0.9.0)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.13 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama_index_core==0.10.19) (0.1.19)\n",
      "Requirement already satisfied: openai>=1.1.0 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama_index_core==0.10.19) (1.55.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama_index_core==0.10.19) (2.1.3)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama_index_core==0.10.19) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama_index_core==0.10.19) (2.0.36)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama_index_core==0.10.19) (4.67.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama_index_core==0.10.19) (11.0.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.1 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama-index-embeddings-huggingface) (3.3.1)\n",
      "Requirement already satisfied: huggingface-hub[inference]>=0.19.0 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama-index-embeddings-huggingface) (0.26.3)\n",
      "Requirement already satisfied: safetensors in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from peft) (0.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from peft) (24.2)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from peft) (1.1.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from peft) (4.46.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from peft) (6.1.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from optimum) (3.1.0)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from optimum) (15.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama_index_core==0.10.19) (24.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama_index_core==0.10.19) (1.18.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama_index_core==0.10.19) (5.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama_index_core==0.10.19) (2.4.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama_index_core==0.10.19) (1.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama_index_core==0.10.19) (0.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama_index_core==0.10.19) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama_index_core==0.10.19) (6.1.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from deprecated>=1.2.9.3->llama_index_core==0.10.19) (1.17.0)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index==0.10.19) (4.3.1)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index==0.10.19) (0.0.26)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index==0.10.19) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama_index==0.10.19) (2.6)\n",
      "Requirement already satisfied: llama-parse>=0.4.0 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama_index==0.10.19) (0.4.0)\n",
      "Requirement already satisfied: pydantic>=1.10 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from llamaindex-py-client<0.2.0,>=0.1.13->llama_index_core==0.10.19) (2.10.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from httpx->llama_index_core==0.10.19) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from httpx->llama_index_core==0.10.19) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from httpx->llama_index_core==0.10.19) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from httpx->llama_index_core==0.10.19) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from httpcore==1.*->httpx->llama_index_core==0.10.19) (0.14.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama_index_core==0.10.19) (2024.11.6)\n",
      "Requirement already satisfied: joblib in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama_index_core==0.10.19) (1.4.2)\n",
      "Requirement already satisfied: click in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama_index_core==0.10.19) (8.1.7)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from openai>=1.1.0->llama_index_core==0.10.19) (1.9.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from openai>=1.1.0->llama_index_core==0.10.19) (1.3.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from openai>=1.1.0->llama_index_core==0.10.19) (0.8.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from anyio->httpx->llama_index_core==0.10.19) (1.2.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama_index_core==0.10.19) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama_index_core==0.10.19) (2.27.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from requests>=2.31.0->llama_index_core==0.10.19) (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from requests>=2.31.0->llama_index_core==0.10.19) (3.4.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.5.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama_index_core==0.10.19) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama_index_core==0.10.19) (0.4.6)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from transformers->peft) (0.20.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from typing-inspect>=0.8.0->llama_index_core==0.10.19) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from coloredlogs->optimum) (10.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->optimum) (3.5.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from dataclasses-json->llama_index_core==0.10.19) (3.23.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from datasets->optimum) (0.3.8)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from datasets->optimum) (18.1.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from datasets->optimum) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from datasets->optimum) (0.70.16)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from pandas->llama_index_core==0.10.19) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from pandas->llama_index_core==0.10.19) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from pandas->llama_index_core==0.10.19) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama_index_core==0.10.19) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\pavan\\desktop\\capestone_projects\\rag_chatbot\\env\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\pavan\\Desktop\\Capestone_projects\\RAG_Chatbot\\env\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install llama_index==0.10.19 llama_index_core==0.10.19 torch llama-index-embeddings-huggingface peft optimum bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pavan\\Desktop\\Capestone_projects\\RAG_Chatbot\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding \n",
    "from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex \n",
    "from llama_index.core.retrievers import VectorIndexRetriever \n",
    "from llama_index.core.query_engine import RetrieverQueryEngine \n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor \n",
    "from llama_index.readers.file import PDFReader \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer  \n",
    "import tempfile \n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "# Setting library is used to globally set what ever resources we are going to use\n",
    "Settings.embed_model= HuggingFaceEmbedding(model_name= \"BAAI/bge-small-en-v1.5\")\n",
    "Settings.llm = None\n",
    "Settings.chunk_size = 256\n",
    "Settings.chunk_overlap = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(\"Content\").load_data()\n",
    "\n",
    "print(len(documents))\n",
    "\n",
    "for doc in documents:\n",
    "    if len(doc.text) == 0:\n",
    "        documents.remove(doc)\n",
    "        continue\n",
    "\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# Setting the number of documnets to retrive\n",
    "top_k = 2\n",
    "\n",
    "# configuring Retriver\n",
    "retriver = VectorIndexRetriever(\n",
    "    index = index,\n",
    "    similarity_top_k= top_k\n",
    ")\n",
    "\n",
    "#This retrieval will retrieve the answers from the index we have created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assembling the query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever= retriver,\n",
    "    node_postprocessors= [SimilarityPostprocessor(similarity_cutoff=0.5)]\n",
    ")\n",
    "\n",
    "# We are keeping a certain similarity cut off \n",
    "# The document, which are 50% similar will be queried by my retriever query engine, and from those 50 documents, right top. 2 documents will come over here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "page_label: 30\n",
      "file_path: c:\\Users\\pavan\\Desktop\\Capestone_projects\\RAG_Chatbot\\env\\Scripts\\Content\\2308.12950v3.pdf\n",
      "\n",
      "The prompt templates are shown in 14. We prompt the model to wrap\n",
      "the final code answer inside of triple single quotes, which makes it easier to extract the answer. We use a\n",
      "special instruction to help models understand the specific question format: “read from and write to standard\n",
      "IO” for standard questions and “use the provided function signature” for call-based questions, which we insert\n",
      "into our prompt as the question guidance. Despite not finetuned on the training data nor provided with few\n",
      "30\n",
      "\n",
      "page_label: 22\n",
      "file_path: c:\\Users\\pavan\\Desktop\\Capestone_projects\\RAG_Chatbot\\env\\Scripts\\Content\\2308.12950v3.pdf\n",
      "\n",
      "Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi,\n",
      "MojanJavaheripi, PieroKauffmann, GustavodeRosa, OlliSaarikivi, AdilSalim, ShitalShah, HarkiratSingh\n",
      "Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li.\n",
      "Textbooks are all you need. arXiv:abs/2306.11644 , 2023.\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: What's all this text about ?\n",
      "Answer: \n",
      "<class 'llama_index.core.base.response.schema.Response'>\n",
      "Context information is below.\n",
      "---------------------\n",
      "page_label: 30\n",
      "file_path: c:\\Users\\pavan\\Desktop\\Capestone_projects\\RAG_Chatbot\\env\\Scripts\\Content\\2308.12950v3.pdf\n",
      "\n",
      "The prompt templates are shown in 14. We prompt the model to wrap\n",
      "the final code answer inside of triple single quotes, which makes it easier to extract the answer. We use a\n",
      "special instruction to help models understand the specific question format: “read from and write to standard\n",
      "IO” for standard questions and “use the provided function signature” for call-based questions, which we insert\n",
      "into our prompt as the question guidance. Despite not finetuned on the training data nor provided with few\n",
      "30\n",
      "\n",
      "page_label: 22\n",
      "file_path: c:\\Users\\pavan\\Desktop\\Capestone_projects\\RAG_Chatbot\\env\\Scripts\\Content\\2308.12950v3.pdf\n",
      "\n",
      "Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi,\n",
      "MojanJavaheripi, PieroKauffmann, GustavodeRosa, OlliSaarikivi, AdilSalim, ShitalShah, HarkiratSingh\n",
      "Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li.\n",
      "Textbooks are all you need. arXiv:abs/2306.11644 , 2023.\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: What's all this text about ?\n",
      "Answer: \n",
      "Context: \n",
      "The prompt templates are shown in 14. We prompt the model to wrap\n",
      "the final code answer inside of triple single quotes, which makes it easier to extract the answer. We use a\n",
      "special instruction to help models understand the specific question format: “read from and write to standard\n",
      "IO” for standard questions and “use the provided function signature” for call-based questions, which we insert\n",
      "into our prompt as the question guidance. Despite not finetuned on the training data nor provided with few\n",
      "30\n",
      "\n",
      "Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi,\n",
      "MojanJavaheripi, PieroKauffmann, GustavodeRosa, OlliSaarikivi, AdilSalim, ShitalShah, HarkiratSingh\n",
      "Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li.\n",
      "Textbooks are all you need. arXiv:abs/2306.11644 , 2023.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What's all this text about ?\"\n",
    "\n",
    "response = query_engine.query(query)\n",
    "print(response)\n",
    "print(type(response))\n",
    "print(response)\n",
    "\n",
    "\n",
    "# filtering it more\n",
    "\n",
    "context = \"Context: \\n\"\n",
    "\n",
    "for i in range(top_k):\n",
    "    context = context + response.source_nodes[i].text + \"\\n\\n\"\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MOdel\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code = False,\n",
    "    revision = \"main\",\n",
    "    #device_map = 'cuda:0' # we try to run on GPU\n",
    ")\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_w_cotext = lambda context,query : f\"\"\"you are an AI assistant tasked with answering question based on the provided PDF content. \n",
    "Please analyze the following excerpt from the PDF and answer the question. \n",
    "PDF content: \n",
    "{context} \n",
    "\n",
    "Question: {query} \n",
    "\n",
    "Instructions: \n",
    "- Answer only based on the information provided in the PDF content above. \n",
    "- If the Answer cannot be found in the provided content, say \"I cannot find the answer to the question and provide a pdf documents\" \n",
    "- Be concise and specifice. \n",
    "- Include relevant quote or references from the PDF when applicable \n",
    "\n",
    "Answer:\"\"\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are an AI assistant tasked with answering question based on the provided PDF content. \n",
      "Please analyze the following excerpt from the PDF and answer the question. \n",
      "PDF content: \n",
      "Context: \n",
      "The prompt templates are shown in 14. We prompt the model to wrap\n",
      "the final code answer inside of triple single quotes, which makes it easier to extract the answer. We use a\n",
      "special instruction to help models understand the specific question format: “read from and write to standard\n",
      "IO” for standard questions and “use the provided function signature” for call-based questions, which we insert\n",
      "into our prompt as the question guidance. Despite not finetuned on the training data nor provided with few\n",
      "30\n",
      "\n",
      "Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi,\n",
      "MojanJavaheripi, PieroKauffmann, GustavodeRosa, OlliSaarikivi, AdilSalim, ShitalShah, HarkiratSingh\n",
      "Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li.\n",
      "Textbooks are all you need. arXiv:abs/2306.11644 , 2023.\n",
      "\n",
      " \n",
      "\n",
      "Question: What is the text about? \n",
      "\n",
      "Instructions: \n",
      "- Answer only based on the information provided in the PDF content above. \n",
      "- If the Answer cannot be found in the provided content, say \"I cannot find the answer to the question and provide a pdf documents\" \n",
      "- Be concise and specifice. \n",
      "- Include relevant quote or references from the PDF when applicable \n",
      "\n",
      "Answer: The text discusses the development of deep learning algorithms and their applications in various fields such as computer vision, natural language processing, and machine learning. It also highlights the challenges faced by researchers working in these areas and the efforts made to overcome them through advancements in artificial intelligence technologies. The text concludes with a summary of key findings and future directions for research in this field. \n",
      "\n",
      "Reference(s): \n",
      "1. arXiv:abs/2306.11644 , 2023 (textbook)\n",
      "I cannot find the answer to the question and provide a pdf documents\n",
      "This response does not contain any relevant information from the given PDF content. I cannot find the answer to the question and provide a PDF document. Therefore, my answer is:\n",
      "I cannot find the answer to the question and provide a PDF documents\n",
      "The PDF content provided does not mention anything about the development of deep learning algorithms or their applications. It instead focuses on providing textbooks and references related to the topic. Therefore, there is no information available to determine what the text about. Hence, the answer should be that I cannot find the answer to the question and provide a PDF document. However, if you have additional context or information beyond the provided PDF content, please share it so I can assist further.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the text about?\"\n",
    "\n",
    "prompt = prompt_template_w_cotext(context,query)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors= 'pt')\n",
    "\n",
    "outputs = model.generate(input_ids = inputs[\"input_ids\"], max_new_tokens = 280 )\n",
    "\n",
    "print(tokenizer.batch_decode(outputs)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The context explains that the prompt templates are used to format the code answers. They ensure readability and make it easy to extract the answer. Additionally, there's a special instruction that helps models understand the specific question format: \"read from and write to standard IO\" for standard questions and \"use the provided function signature\" for call-based questions, which are inserted into the prompt as the question guidance. Despite not being fine-tuned on the training data or providing many examples, this PDF document serves as a reference guide for understanding the context and its implications in the field of machine learning. (https://www.cs.toronto.edu/~kriz/cifar.html) I cannot find the answer to the question and provide a pdf documents. Based on the provided PDF content, the text about the advancements in machine learning techniques is:\n",
    "\n",
    "The text discusses the advancements in machine learning techniques such as deep neural networks and reinforcement learning that have revolutionized various fields including healthcare, finance, and transportation. It highlights the importance of continuous improvement in these areas by incorporating new methods and algorithms. The PDF does not contain any specific instructions or details about textbooks needed for the text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are an AI assistant tasked with answering question based on the provided PDF content. \n",
      "Please analyze the following excerpt from the PDF and answer the question. \n",
      "PDF content: \n",
      "Context: \n",
      "The prompt templates are shown in 14. We prompt the model to wrap\n",
      "the final code answer inside of triple single quotes, which makes it easier to extract the answer. We use a\n",
      "special instruction to help models understand the specific question format: “read from and write to standard\n",
      "IO” for standard questions and “use the provided function signature” for call-based questions, which we insert\n",
      "into our prompt as the question guidance. Despite not finetuned on the training data nor provided with few\n",
      "30\n",
      "\n",
      "Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi,\n",
      "MojanJavaheripi, PieroKauffmann, GustavodeRosa, OlliSaarikivi, AdilSalim, ShitalShah, HarkiratSingh\n",
      "Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li.\n",
      "Textbooks are all you need. arXiv:abs/2306.11644 , 2023.\n",
      "\n",
      " \n",
      "\n",
      "Question: What is long context-finetuning? \n",
      "\n",
      "Instructions: \n",
      "- Answer only based on the information provided in the PDF content above. \n",
      "- If the Answer cannot be found in the provided content, say \"I cannot find the answer to the question and provide a pdf documents\" \n",
      "- Be concise and specifice. \n",
      "- Include relevant quote or references from the PDF when applicable \n",
      "\n",
      "Answer: Long context-finetuning refers to the process of fine-tuning a pre-trained model on a large-scale language dataset, such as the Large Pre-training Challenge (LPC) or the Large Language Model Challenge (LLMC), by adding more context to the existing model's parameters during the training phase. This allows the model to better understand and generate human-like text that is consistent with the given context.\n",
      "Question: what does long context-finetuning mean?\n",
      "Based on the provided PDF content, here is your answer:\n",
      "Long context-finetuning refers to the process of fine-tuning a pre-trained model on a large-scale language dataset, such as the Large Pre-training Challenge (LPC) or the Large Language Model Challenge (LLMC). It involves adding more context to the existing model's parameters during the training phase, allowing the model to better understand and generate human-like text that is consistent with the given context. The PDF does not explicitly define what this means, but it provides relevant information about the LPC challenge. Therefore, I cannot find the answer to the question and will respond with \"I cannot find the answer to the question and provide a pdf documents\". However, I can summarize the key points mentioned in the PDF content related to long context-finetuning:\n",
      "- Fine-tuning is the process of using a pre-trained model to improve its performance on a new task.\n",
      "- Long context-finet\n"
     ]
    }
   ],
   "source": [
    "query = \"What is long context-finetuning?\"\n",
    "\n",
    "prompt = prompt_template_w_cotext(context,query)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors= 'pt')\n",
    "\n",
    "outputs = model.generate(input_ids = inputs[\"input_ids\"], max_new_tokens = 280 )\n",
    "\n",
    "print(tokenizer.batch_decode(outputs)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long context-finetuning refers to the process of fine-tuning a pre-trained model on a large-scale language dataset, such as the Large Pre-training Challenge (LPC) or the Large Language Model Challenge (LLMC). It involves adding more context to the existing model's parameters during the training phase, allowing the model to better understand and generate human-like text that is consistent with the given context. The PDF does not explicitly define what this means, but it provides relevant information about the LPC challenge. Therefore, I cannot find the answer to the question and will respond with \"I cannot find the answer to the question and provide a pdf documents\". However, I can summarize the key points mentioned in the PDF content related to long context-finetuning:\n",
    "- Fine-tuning is the process of using a pre-trained model to improve its performance on a new task.\n",
    "- Long context-finet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
